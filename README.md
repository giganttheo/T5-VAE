# T5-VAE

For the [Flax/JAX Community Week](https://github.com/huggingface/transformers/tree/main/examples/research_projects/jax-projects), I worked on the T5-VAE project that combines a T5 transformer model with a variational autoencoder to learn smooth latent spaces for texts.

![T5-VAE](https://github.com/giganttheo/T5-VAE/blob/master/t5-vae.png?raw=true)

### Links :

* [Model card](https://huggingface.co/flax-community/t5-vae-wiki) for the T5-VAE trained on Wikipedia sentences.
* [Model card](https://huggingface.co/flax-community/t5-vae-python) for the T5-VAE trained on python code.
* Fraser Greenlee's article for [making a Transformer-VAE with JAX](https://fras.uk/ml/transformer-vae/2021/06/13/Making-a-Transformer-VAE-with-JAX.html)
* Fraser Greenlee's article about [Transformers as Variational Autoencoders
](https://fras.uk/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html)
